{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpfwcMJHTtfw"
      },
      "source": [
        "\n",
        "# Character-level Transformer on Tiny Shakespeare\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/main/examples/nanolm.ipynb)\n",
        "\n",
        "This example demonstrates how to train a small-scale transformer-based language model (inspired by NanoGPT) on the Tiny Shakespeare dataset.  The core idea is to train a model that can predict the next character in a sequence of text based on the characters that came before it.\n",
        "\n",
        "**Why the Tiny Shakespeare Dataset?**\n",
        "\n",
        "* **Manageable Size:**  Since we're building a small-scale model, the Tiny Shakespeare dataset provides a suitable training corpus without overwhelming computational resources.\n",
        "* **Linguistic Complexity:** Shakespeare's works offer a rich vocabulary and interesting grammatical patterns, making the dataset a good testbed for our model's language learning abilities.\n",
        "* **Accessibility:** Easily accessible through [TensorFlow Datasets](https://www.tensorflow.org/datasets).\n",
        "\n",
        "**Libraries Used**\n",
        "\n",
        "* **JAX:** Provides the foundation for numerical computations and automatic differentiation.\n",
        "* **Tensorflow Datasets (`tfds`)** Offers easy access to the Tiny Shakespeare dataset.\n",
        "* **Flax's Linen Module:** Provides building blocks for defining our neural network architecture.\n",
        "* **Optax:** Contains a library of optimization algorithms for training the model's parameters. In this example we'll use the {py:func}`optax.adamw` solver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIabArrRWFw0",
        "outputId": "75c89cb3-35ca-4217-a8e0-4b45f9efe31f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# platform check\n",
        "print(\"JAX running on\", jax.devices()[0].platform.upper())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhFD-uojcAI6"
      },
      "source": [
        "# Hyperparameters and dataset download\n",
        "\n",
        "Next, we set some important hyperparameters. This includes hyperparameters for the training process such as the learning rate `LEARNING_RATE` and the batch size `BATCH_SIZE`, as well as model parameters such as the context window size `BLOCK_SIZE` and the number of layers `NUM_LAYERS`.\n",
        "\n",
        "\n",
        "After setting these, we load the Tiny Shakespeare dataset and print the length of the training set, which is around one million characters, and that of the validation set (around 50k characters). Finally, we print a small snippet of the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34pKN_bIXt8O"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# @markdown Random seed:\n",
        "SEED = 42  # @param{type:\"integer\"}\n",
        "# @markdown Learning rate passed to the optimizer:\n",
        "LEARNING_RATE = 5e-3 # @param{type:\"number\"}\n",
        "# @markdown Batch size:\n",
        "BATCH_SIZE = 128  # @param{type:\"integer\"}\n",
        "# @markdown Numer of training iterations:\n",
        "N_ITERATIONS = 50_000  # @param{type:\"integer\"}\n",
        "# @markdown Number of training iterations between two consecutive evaluations:\n",
        "N_FREQ_EVAL = 2_000 # @param{type:\"integer\"}\n",
        "# @markdown Rate for dropout in the transformer model\n",
        "DROPOUT_RATE = 0.2  # @param{type:\"number\"}\n",
        "# @markdown Context window for the transformer model\n",
        "BLOCK_SIZE = 64  # @param{type:\"integer\"}\n",
        "# @markdown Number of layer for the transformer model\n",
        "NUM_LAYERS = 6  # @param{type:\"integer\"}\n",
        "# @markdown Size of the embedding for the transformer model\n",
        "EMBED_SIZE = 256  # @param{type:\"integer\"}\n",
        "# @markdown Number of heads for the transformer model\n",
        "NUM_HEADS = 8  # @param{type:\"integer\"}\n",
        "# @markdown Size of the heads for the transformer model\n",
        "HEAD_SIZE = 32  # @param{type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mghpbB9653Gw"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "ds = tfds.load(\"tiny_shakespeare\")\n",
        "\n",
        "# combine train and test examples into a single string\n",
        "text_train = \"\"\n",
        "for example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n",
        "  text_train += example[\"text\"].decode(\"utf-8\")\n",
        "\n",
        "# similarly, create a single string for validation\n",
        "text_validation = \"\"\n",
        "for example in ds[\"validation\"].as_numpy_iterator():\n",
        "  text_validation += example[\"text\"].decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USiJ0GjWSPu_",
        "outputId": "bd033bb3-1c6a-4dc4-a1e3-b38a04dd43c0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(f\"Length of text for training: {len(text_train):_} characters\")\n",
        "print(f\"Length of text for validation: {len(text_validation):_} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOq-djQ9cueI",
        "outputId": "8d639dad-b1df-4536-8f74-bed86a869201"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# small sample of the train set\n",
        "print(text_train[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FguiERfTcEPa"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "To prepare the data for the model, we first create a vocabulary consisting of all the unique characters in the dataset. We print that vocabulary and its size.\n",
        "\n",
        "We then define encoding and decoding functions to convert text into sequences of integers (representing our characters) and vice versa.\n",
        "\n",
        "Finally, we define a function `get_batch` that returns random mini-batches of data. This function uses JAX's {py:func}`jax.lax.dynamic_slice` function to efficiently handle sequences of varying lengths within batches. The `@jax.jit` decorator compiles this function for faster execution. The function randomly samples a batch from the data and prepares input sequences (`x`) and target sequences (`y`). The target sequence is simply the input sequence shifted by one position, as the goal of the language model is to predict the next character given the previous ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rESkNoDXFE-4",
        "outputId": "42579042-716c-4101-cf08-14ebefa6c984"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "vocab = sorted(list(set(text_train)))\n",
        "print(\"Vocabulary:, \", \"\".join(vocab))\n",
        "print(\"Length of vocabulary: \", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-LSTr86bXrV"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s: [\n",
        "    stoi[c] for c in s\n",
        "]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join(\n",
        "    [itos[i] for i in l]\n",
        ")  # decoder: take a list of integers, output a string\n",
        "\n",
        "# encode train and validation data\n",
        "train_data = jnp.array(encode(text_train))\n",
        "eval_data = jnp.array(encode(text_validation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZLP1asmb8WY"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def get_batch(random_key, data):\n",
        "  \"\"\"Prepares a random batch of training data.\n",
        "\n",
        "  Args:\n",
        "      random_key: A random seed for sampling a batch.\n",
        "      data: The complete training dataset.\n",
        "\n",
        "  Returns:\n",
        "      x: Input sequences.\n",
        "      y: Target sequences (shifted inputs).\n",
        "  \"\"\"\n",
        "  ix = jax.random.randint(\n",
        "      random_key, shape=(BATCH_SIZE, 1), minval=0, maxval=len(data) - BLOCK_SIZE\n",
        "  )\n",
        "  x = dynamic_slice_vmap(data, ix, (BLOCK_SIZE,))\n",
        "  y = dynamic_slice_vmap(data, ix + 1, (BLOCK_SIZE,))\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQfTaf2UcTSc"
      },
      "source": [
        "# NanoLM Model Definition\n",
        "\n",
        "The NanoLM model itself is defined as a Flax Linen module.  The core of the model is a Transformer architecture, designed for sequence-to-sequence tasks like language modeling. Key parameters of the model, such as the number of layers, attention heads, and embedding size, are specified here.\n",
        "\n",
        "Inside the model's `__call__` method, we first embed our input characters into vector representations. Positional embeddings are added to provide the model with a sense of order in the sequence. The core of the Transformer consists of multiple layers. Each layer has two main components:\n",
        "\n",
        " * **Multi-Head Attention**: This mechanism allows the model to \"attend\" to different parts of the input sequence, improving its understanding of context and relationships within the text. In the code this is implemented through the {py:class}`flax.linen.MultiHeadDotProductAttention` class.\n",
        "\n",
        " * **Feedforward Network**: This network processes the output of the attention layer, applying non-linear transformations to further learn complex patterns in the data. This is implemented through the {py:class}`flax.linen.Sequential` class.\n",
        "\n",
        "Normalization and dropout (for regularization) are used within the layers to improve training stability.  Finally, a dense layer maps the model's output to the vocabulary size, producing probabilities for each character as the next potential character.\n",
        "\n",
        "The generate function enables the model to create new text sequences. It iteratively generates one character at a time, conditioned on the previously generated text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c7M35UaYMyD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "class NanoLM(nn.Module):\n",
        "  \"\"\"NanoLM model.\"\"\"\n",
        "  vocab_size: int\n",
        "  num_layers: int = 6\n",
        "  num_heads: int = 8\n",
        "  head_size: int = 32\n",
        "  dropout_rate: float = 0.2\n",
        "  embed_size: int = 256\n",
        "  block_size: int = 64\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, training: bool):\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n",
        "        self.block_size, self.embed_size\n",
        "    )(jnp.arange(seq_len))\n",
        "    for _ in range(self.num_layers):\n",
        "      x_norm = nn.LayerNorm()(x)\n",
        "      x = x + nn.MultiHeadDotProductAttention(\n",
        "          num_heads=self.num_heads,\n",
        "          qkv_features=self.head_size,\n",
        "          out_features=self.head_size * self.num_heads,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "      )(\n",
        "          x_norm,\n",
        "          x_norm,\n",
        "          mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n",
        "          deterministic=not training,\n",
        "      )\n",
        "\n",
        "      x = x + nn.Sequential([\n",
        "          nn.Dense(4 * self.embed_size),\n",
        "          nn.relu,\n",
        "          nn.Dropout(self.dropout_rate, deterministic=not training),\n",
        "          nn.Dense(self.embed_size),\n",
        "      ])(nn.LayerNorm()(x))\n",
        "\n",
        "    x = nn.LayerNorm()(x)\n",
        "    return nn.Dense(self.vocab_size)(x)\n",
        "\n",
        "  @functools.partial(jax.jit, static_argnames=(\"self\", \"length\"))\n",
        "  def generate(self, rng, params, length):\n",
        "    def _scan_generate(carry, _):\n",
        "      random_key, context = carry\n",
        "      logits = self.apply(params, context, training=False)\n",
        "      rng, rng_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(\n",
        "          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n",
        "      )\n",
        "      context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n",
        "      return (rng, context), new_token\n",
        "\n",
        "    _, new_tokens = jax.lax.scan(\n",
        "        _scan_generate,\n",
        "        (rng, jnp.zeros((1, self.block_size), dtype=jnp.int32)),\n",
        "        (),\n",
        "        length=length,\n",
        "    )\n",
        "    return new_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylVNKEhscy9d"
      },
      "source": [
        "# State, Optimizer, and Loss Definition\n",
        "\n",
        "This section initializes the model's parameters, defines the loss function used for language modeling, and sets up the training and evaluation processes.\n",
        "\n",
        "In this case the loss function `loss_fun` is the cross-entropy. It uses dropout for regularization, introduced via the `rngs={\"dropout\": dropout_key}` argument. We also define a function for evaluating the model's performance on unseen data (`eval_step`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjSnK3yDYIus"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "model = NanoLM(\n",
        "    vocab_size=len(vocab),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    head_size=HEAD_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    block_size=BLOCK_SIZE,\n",
        ")\n",
        "\n",
        "def loss_fun(params, x, y, dropout_key):\n",
        "  logits = model.apply(params, x, training=True, rngs={\"dropout\": dropout_key})\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "  logits = model.apply(params, x, training=False)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejU1Yt8XIH80"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(SEED)\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "var_params = model.init(\n",
        "    key,\n",
        "    jnp.ones((BATCH_SIZE, BLOCK_SIZE), dtype=jnp.int32),\n",
        "    training=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgSjWONs4eFp"
      },
      "source": [
        "We've now instatiated a NanoLM model with the following number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ckqdkd6QVsl",
        "outputId": "2c202753-b938-4148-992c-cc63077be607"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "n_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n",
        "\n",
        "print(f\"Total number of parameters: {n_params:_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreyB_oo4Zch"
      },
      "source": [
        "# Model training\n",
        "\n",
        "We start by creating an optimizer and instantiating its state. In this case we'll use {py:func}`optax.adamw` but it's possible to replace this with other optax optimizers.\n",
        "\n",
        "\n",
        "We then proceeded to the training loop. For maximum efficiency we extracted the most computationally intensive tasks inside the `step` function and just-in-time compile this function using `@jax.jit`. This allows JAX to perform some optimizations in our code and generally achieve a much higher efficiency than without.\n",
        "\n",
        "Inside the training loop, we call the aforementioned `step` functions, as well as computing accuracy on a validation set every `N_FREQ_EVAL` iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xwLpjDxccMi"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# To run with SGD instead of adam, replace `adam` with `sgd`\n",
        "opt = optax.adamw(learning_rate=LEARNING_RATE)\n",
        "\n",
        "opt_state = opt.init(var_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhnK0G7AQUCA",
        "outputId": "37b5a3bd-c0bc-47c2-a828-62356ab4cbe5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "all_train_losses = []\n",
        "all_eval_losses = []\n",
        "\n",
        "# we define one iteration of the optimizer and JIT this function\n",
        "@jax.jit\n",
        "def step(key, params, opt_state):\n",
        "  key, subkey = jax.random.split(key)\n",
        "  batch = get_batch(key, train_data)\n",
        "  loss, grad = jax.value_and_grad(loss_fun)(params, *batch, subkey)\n",
        "  updates, opt_state = opt.update(grad, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, key, opt_state, loss\n",
        "\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "  var_params, key, opt_state, loss = step(key, var_params, opt_state)\n",
        "  all_train_losses.append(loss)\n",
        "\n",
        "  # once every N_FREQ_EVAL we compute loss on the validation set\n",
        "  if i % N_FREQ_EVAL == 0:\n",
        "    key, subkey = jax.random.split(key)\n",
        "    eval_loss = eval_step(var_params, *get_batch(subkey, eval_data))\n",
        "    all_eval_losses.append(eval_loss)\n",
        "    print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Gc-V4kAKAA9q",
        "outputId": "68b59bf2-9621-4c2e-e0e0-be665d19630e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "plt.title(f\"Convergence of adamw (train loss)\")\n",
        "plt.plot(all_train_losses, label=\"train\", lw=3)\n",
        "plt.plot(\n",
        "    jnp.arange(0, len(all_eval_losses) * N_FREQ_EVAL, N_FREQ_EVAL),\n",
        "    all_eval_losses,\n",
        "    label=\"test\",\n",
        "    lw=3,\n",
        ")\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.grid()\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6-aaLDL7RbI"
      },
      "source": [
        "# Text generation\n",
        "\n",
        "Finally, after training, we use the generate function to let the NanoGPT model demonstrate its ability to create text that resembles Shakespeare, albeit in a miniature form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AejKtZnFmhK",
        "outputId": "78e7b13f-28ff-4786-e206-bcd104e3c507"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'Python 3.11.5' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Let's now generate some text\n",
        "key, subkey = jax.random.split(key)\n",
        "text = model.generate(key, var_params, 1000)[:, 0, 0].tolist()\n",
        "print(decode(text))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
