{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpfwcMJHTtfw"
      },
      "source": [
        "\n",
        "# Character-level Transformer on Tiny Shakespeare\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/main/examples/nanolm.ipynb)\n",
        "\n",
        "This example demonstrates how to train a small-scale transformer-based language model (inspired by NanoGPT) on the Tiny Shakespeare dataset.  The core idea is to train a model that can predict the next character in a sequence of text based on the characters that came before it.\n",
        "\n",
        "**Why the Tiny Shakespeare Dataset?**\n",
        "\n",
        "* **Manageable Size:**  Since we're building a small-scale model, the Tiny Shakespeare dataset provides a suitable training corpus without overwhelming computational resources.\n",
        "* **Linguistic Complexity:** Shakespeare's works offer a rich vocabulary and interesting grammatical patterns, making the dataset a good testbed for our model's language learning abilities.\n",
        "* **Accessibility:** Easily accessible through [TensorFlow Datasets](https://www.tensorflow.org/datasets).\n",
        "\n",
        "**Libraries Used**\n",
        "\n",
        "* **JAX:** Provides the foundation for numerical computations and automatic differentiation.\n",
        "* **Tensorflow Datasets (`tfds`)** Offers easy access to the Tiny Shakespeare dataset.\n",
        "* **Flax's Linen Module:** Provides building blocks for defining our neural network architecture.\n",
        "* **Optax:** Contains a library of optimization algorithms for training the model's parameters. In this example we'll use the {py:func}`optax.adamw` solver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIabArrRWFw0",
        "outputId": "75c89cb3-35ca-4217-a8e0-4b45f9efe31f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/i-sugiura/jax-llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX running on CPU\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import pyplot as plt\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# platform check\n",
        "print(\"JAX running on\", jax.devices()[0].platform.upper())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhFD-uojcAI6"
      },
      "source": [
        "# Hyperparameters and dataset download\n",
        "\n",
        "Next, we set some important hyperparameters. This includes hyperparameters for the training process such as the learning rate `LEARNING_RATE` and the batch size `BATCH_SIZE`, as well as model parameters such as the context window size `BLOCK_SIZE` and the number of layers `NUM_LAYERS`.\n",
        "\n",
        "\n",
        "After setting these, we load the Tiny Shakespeare dataset and print the length of the training set, which is around one million characters, and that of the validation set (around 50k characters). Finally, we print a small snippet of the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "34pKN_bIXt8O"
      },
      "outputs": [],
      "source": [
        "# @markdown Random seed:\n",
        "SEED = 42  # @param{type:\"integer\"}\n",
        "# @markdown Learning rate passed to the optimizer:\n",
        "LEARNING_RATE = 5e-3 # @param{type:\"number\"}\n",
        "# @markdown Batch size:\n",
        "BATCH_SIZE = 128  # @param{type:\"integer\"}\n",
        "# @markdown Numer of training iterations:\n",
        "N_ITERATIONS = 50_000  # @param{type:\"integer\"}\n",
        "# @markdown Number of training iterations between two consecutive evaluations:\n",
        "N_FREQ_EVAL = 2_000 # @param{type:\"integer\"}\n",
        "# @markdown Rate for dropout in the transformer model\n",
        "DROPOUT_RATE = 0.2  # @param{type:\"number\"}\n",
        "# @markdown Context window for the transformer model\n",
        "BLOCK_SIZE = 64  # @param{type:\"integer\"}\n",
        "# @markdown Number of layer for the transformer model\n",
        "NUM_LAYERS = 6  # @param{type:\"integer\"}\n",
        "# @markdown Size of the embedding for the transformer model\n",
        "EMBED_SIZE = 256  # @param{type:\"integer\"}\n",
        "# @markdown Number of heads for the transformer model\n",
        "NUM_HEADS = 8  # @param{type:\"integer\"}\n",
        "# @markdown Size of the heads for the transformer model\n",
        "HEAD_SIZE = 32  # @param{type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mghpbB9653Gw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-12 13:38:58.409683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-12 13:39:01.864310: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: 1.06 MiB, total: 1.06 MiB) to /home/i-sugiura/tensorflow_datasets/tiny_shakespeare/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dl Size...: 1 MiB [00:00,  2.19 MiB/s][00:00<00:00,  2.27 url/s]\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.18 url/s]\n",
            "2024-05-12 13:39:10.979021: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset tiny_shakespeare downloaded and prepared to /home/i-sugiura/tensorflow_datasets/tiny_shakespeare/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-12 13:39:11.240842: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-05-12 13:39:11.287541: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "ds = tfds.load(\"tiny_shakespeare\")\n",
        "\n",
        "# combine train and test examples into a single string\n",
        "text_train = \"\"\n",
        "for example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n",
        "  text_train += example[\"text\"].decode(\"utf-8\")\n",
        "\n",
        "# similarly, create a single string for validation\n",
        "text_validation = \"\"\n",
        "for example in ds[\"validation\"].as_numpy_iterator():\n",
        "  text_validation += example[\"text\"].decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USiJ0GjWSPu_",
        "outputId": "bd033bb3-1c6a-4dc4-a1e3-b38a04dd43c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text for training: 1_059_624 characters\n",
            "Length of text for validation: 55_770 characters\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of text for training: {len(text_train):_} characters\")\n",
        "print(f\"Length of text for validation: {len(text_validation):_} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOq-djQ9cueI",
        "outputId": "8d639dad-b1df-4536-8f74-bed86a869201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# small sample of the train set\n",
        "print(text_train[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FguiERfTcEPa"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "To prepare the data for the model, we first create a vocabulary consisting of all the unique characters in the dataset. We print that vocabulary and its size.\n",
        "\n",
        "We then define encoding and decoding functions to convert text into sequences of integers (representing our characters) and vice versa.\n",
        "\n",
        "Finally, we define a function `get_batch` that returns random mini-batches of data. This function uses JAX's {py:func}`jax.lax.dynamic_slice` function to efficiently handle sequences of varying lengths within batches. The `@jax.jit` decorator compiles this function for faster execution. The function randomly samples a batch from the data and prepares input sequences (`x`) and target sequences (`y`). The target sequence is simply the input sequence shifted by one position, as the goal of the language model is to predict the next character given the previous ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rESkNoDXFE-4",
        "outputId": "42579042-716c-4101-cf08-14ebefa6c984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:,  \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Length of vocabulary:  65\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(list(set(text_train)))\n",
        "print(\"Vocabulary:, \", \"\".join(vocab))\n",
        "print(\"Length of vocabulary: \", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F-LSTr86bXrV"
      },
      "outputs": [],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s: [\n",
        "    stoi[c] for c in s\n",
        "]  # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join(\n",
        "    [itos[i] for i in l]\n",
        ")  # decoder: take a list of integers, output a string\n",
        "\n",
        "# encode train and validation data\n",
        "train_data = jnp.array(encode(text_train))\n",
        "eval_data = jnp.array(encode(text_validation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tZLP1asmb8WY"
      },
      "outputs": [],
      "source": [
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def get_batch(random_key, data):\n",
        "  \"\"\"Prepares a random batch of training data.\n",
        "\n",
        "  Args:\n",
        "      random_key: A random seed for sampling a batch.\n",
        "      data: The complete training dataset.\n",
        "\n",
        "  Returns:\n",
        "      x: Input sequences.\n",
        "      y: Target sequences (shifted inputs).\n",
        "  \"\"\"\n",
        "  ix = jax.random.randint(\n",
        "      random_key, shape=(BATCH_SIZE, 1), minval=0, maxval=len(data) - BLOCK_SIZE\n",
        "  )\n",
        "  x = dynamic_slice_vmap(data, ix, (BLOCK_SIZE,))\n",
        "  y = dynamic_slice_vmap(data, ix + 1, (BLOCK_SIZE,))\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQfTaf2UcTSc"
      },
      "source": [
        "# NanoLM Model Definition\n",
        "\n",
        "The NanoLM model itself is defined as a Flax Linen module.  The core of the model is a Transformer architecture, designed for sequence-to-sequence tasks like language modeling. Key parameters of the model, such as the number of layers, attention heads, and embedding size, are specified here.\n",
        "\n",
        "Inside the model's `__call__` method, we first embed our input characters into vector representations. Positional embeddings are added to provide the model with a sense of order in the sequence. The core of the Transformer consists of multiple layers. Each layer has two main components:\n",
        "\n",
        " * **Multi-Head Attention**: This mechanism allows the model to \"attend\" to different parts of the input sequence, improving its understanding of context and relationships within the text. In the code this is implemented through the {py:class}`flax.linen.MultiHeadDotProductAttention` class.\n",
        "\n",
        " * **Feedforward Network**: This network processes the output of the attention layer, applying non-linear transformations to further learn complex patterns in the data. This is implemented through the {py:class}`flax.linen.Sequential` class.\n",
        "\n",
        "Normalization and dropout (for regularization) are used within the layers to improve training stability.  Finally, a dense layer maps the model's output to the vocabulary size, producing probabilities for each character as the next potential character.\n",
        "\n",
        "The generate function enables the model to create new text sequences. It iteratively generates one character at a time, conditioned on the previously generated text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-c7M35UaYMyD"
      },
      "outputs": [],
      "source": [
        "class NanoLM(nn.Module):\n",
        "  \"\"\"NanoLM model.\"\"\"\n",
        "  vocab_size: int\n",
        "  num_layers: int = 6\n",
        "  num_heads: int = 8\n",
        "  head_size: int = 32\n",
        "  dropout_rate: float = 0.2\n",
        "  embed_size: int = 256\n",
        "  block_size: int = 64\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, training: bool):\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n",
        "        self.block_size, self.embed_size\n",
        "    )(jnp.arange(seq_len))\n",
        "    for _ in range(self.num_layers):\n",
        "      x_norm = nn.LayerNorm()(x)\n",
        "      x = x + nn.MultiHeadDotProductAttention(\n",
        "          num_heads=self.num_heads,\n",
        "          qkv_features=self.head_size,\n",
        "          out_features=self.head_size * self.num_heads,\n",
        "          dropout_rate=self.dropout_rate,\n",
        "      )(\n",
        "          x_norm,\n",
        "          x_norm,\n",
        "          mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n",
        "          deterministic=not training,\n",
        "      )\n",
        "\n",
        "      x = x + nn.Sequential([\n",
        "          nn.Dense(4 * self.embed_size),\n",
        "          nn.relu,\n",
        "          nn.Dropout(self.dropout_rate, deterministic=not training),\n",
        "          nn.Dense(self.embed_size),\n",
        "      ])(nn.LayerNorm()(x))\n",
        "\n",
        "    x = nn.LayerNorm()(x)\n",
        "    return nn.Dense(self.vocab_size)(x)\n",
        "\n",
        "  @functools.partial(jax.jit, static_argnames=(\"self\", \"length\"))\n",
        "  def generate(self, rng, params, length):\n",
        "    def _scan_generate(carry, _):\n",
        "      random_key, context = carry\n",
        "      logits = self.apply(params, context, training=False)\n",
        "      rng, rng_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(\n",
        "          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n",
        "      )\n",
        "      context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n",
        "      return (rng, context), new_token\n",
        "\n",
        "    _, new_tokens = jax.lax.scan(\n",
        "        _scan_generate,\n",
        "        (rng, jnp.zeros((1, self.block_size), dtype=jnp.int32)),\n",
        "        (),\n",
        "        length=length,\n",
        "    )\n",
        "    return new_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylVNKEhscy9d"
      },
      "source": [
        "# State, Optimizer, and Loss Definition\n",
        "\n",
        "This section initializes the model's parameters, defines the loss function used for language modeling, and sets up the training and evaluation processes.\n",
        "\n",
        "In this case the loss function `loss_fun` is the cross-entropy. It uses dropout for regularization, introduced via the `rngs={\"dropout\": dropout_key}` argument. We also define a function for evaluating the model's performance on unseen data (`eval_step`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sjSnK3yDYIus"
      },
      "outputs": [],
      "source": [
        "model = NanoLM(\n",
        "    vocab_size=len(vocab),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    head_size=HEAD_SIZE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    block_size=BLOCK_SIZE,\n",
        ")\n",
        "\n",
        "def loss_fun(params, x, y, dropout_key):\n",
        "  logits = model.apply(params, x, training=True, rngs={\"dropout\": dropout_key})\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "  logits = model.apply(params, x, training=False)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=y\n",
        "  ).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ejU1Yt8XIH80"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(SEED)\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "var_params = model.init(\n",
        "    key,\n",
        "    jnp.ones((BATCH_SIZE, BLOCK_SIZE), dtype=jnp.int32),\n",
        "    training=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgSjWONs4eFp"
      },
      "source": [
        "We've now instatiated a NanoLM model with the following number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ckqdkd6QVsl",
        "outputId": "2c202753-b938-4148-992c-cc63077be607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 3_408_513\n"
          ]
        }
      ],
      "source": [
        "n_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n",
        "\n",
        "print(f\"Total number of parameters: {n_params:_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreyB_oo4Zch"
      },
      "source": [
        "# Model training\n",
        "\n",
        "We start by creating an optimizer and instantiating its state. In this case we'll use {py:func}`optax.adamw` but it's possible to replace this with other optax optimizers.\n",
        "\n",
        "\n",
        "We then proceeded to the training loop. For maximum efficiency we extracted the most computationally intensive tasks inside the `step` function and just-in-time compile this function using `@jax.jit`. This allows JAX to perform some optimizations in our code and generally achieve a much higher efficiency than without.\n",
        "\n",
        "Inside the training loop, we call the aforementioned `step` functions, as well as computing accuracy on a validation set every `N_FREQ_EVAL` iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1xwLpjDxccMi"
      },
      "outputs": [],
      "source": [
        "# To run with SGD instead of adam, replace `adam` with `sgd`\n",
        "opt = optax.adamw(learning_rate=LEARNING_RATE)\n",
        "\n",
        "opt_state = opt.init(var_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhnK0G7AQUCA",
        "outputId": "37b5a3bd-c0bc-47c2-a828-62356ab4cbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0\t train loss: 4.577892303466797\t eval loss: 5.943662166595459\n",
            "Step: 2000\t train loss: 1.9297360181808472\t eval loss: 1.8235480785369873\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "all_train_losses = []\n",
        "all_eval_losses = []\n",
        "\n",
        "# we define one iteration of the optimizer and JIT this function\n",
        "@jax.jit\n",
        "def step(key, params, opt_state):\n",
        "  key, subkey = jax.random.split(key)\n",
        "  batch = get_batch(key, train_data)\n",
        "  loss, grad = jax.value_and_grad(loss_fun)(params, *batch, subkey)\n",
        "  updates, opt_state = opt.update(grad, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, key, opt_state, loss\n",
        "\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "  var_params, key, opt_state, loss = step(key, var_params, opt_state)\n",
        "  all_train_losses.append(loss)\n",
        "\n",
        "  # once every N_FREQ_EVAL we compute loss on the validation set\n",
        "  if i % N_FREQ_EVAL == 0:\n",
        "    key, subkey = jax.random.split(key)\n",
        "    eval_loss = eval_step(var_params, *get_batch(subkey, eval_data))\n",
        "    all_eval_losses.append(eval_loss)\n",
        "    print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Gc-V4kAKAA9q",
        "outputId": "68b59bf2-9621-4c2e-e0e0-be665d19630e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'.venv (Python 3.11.5)' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/home/i-sugiura/jax-llm/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "plt.title(f\"Convergence of adamw (train loss)\")\n",
        "plt.plot(all_train_losses, label=\"train\", lw=3)\n",
        "plt.plot(\n",
        "    jnp.arange(0, len(all_eval_losses) * N_FREQ_EVAL, N_FREQ_EVAL),\n",
        "    all_eval_losses,\n",
        "    label=\"test\",\n",
        "    lw=3,\n",
        ")\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.grid()\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6-aaLDL7RbI"
      },
      "source": [
        "# Text generation\n",
        "\n",
        "Finally, after training, we use the generate function to let the NanoGPT model demonstrate its ability to create text that resembles Shakespeare, albeit in a miniature form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AejKtZnFmhK",
        "outputId": "78e7b13f-28ff-4786-e206-bcd104e3c507"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m'.venv (Python 3.11.5)' でセルを実行するには、 ipykernel パッケージが必要です。\n",
            "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
            "\u001b[1;31mコマンド: '/home/i-sugiura/jax-llm/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Let's now generate some text\n",
        "key, subkey = jax.random.split(key)\n",
        "text = model.generate(key, var_params, 1000)[:, 0, 0].tolist()\n",
        "print(decode(text))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
